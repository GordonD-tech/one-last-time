# Load libraries
library(tidymodels)
library(caret)
library(MASS)
library(lmridge)
library(Metrics)
library(tidyverse)
library(tseries)
library(ggthemes)
library(ggplot2)
library(glmnet)  # For Elastic Net
library(baguette) #FOR BAGGED TREES
library(ranger) #FOR RANDOM FORESTS
library(randomForest) #ALT RANDOM FOREST PACKAGE
library(xgboost) #FOR GRADIENT BOOSTING
library(vip) #FOR VARIABLE IMPORTANCE

# Load dataset
df <- read.csv('https://raw.githubusercontent.com/GordonD-tech/one-last-time/refs/heads/main/Sleep_health_and_lifestyle_dataset.csv')

# Check correlation
cor_matrix <- cor(df[, sapply(df, is.numeric)], use = "complete.obs")
print(cor_matrix["Stress.Level", ])

# Load SVM library
library(e1071)

# Re-factor entire dataset first to capture all levels
df$Gender <- as.factor(df$Gender)
df$Occupation <- as.factor(df$Occupation)
df$BMI.Category <- as.factor(df$BMI.Category)

# Partition data
set.seed(123)
split1 <- initial_split(df, prop = 0.7, strata = Stress.Level)
train <- training(split1)
holdout <- testing(split1)
split2 <- initial_split(holdout, prop = 0.5, strata = Stress.Level)
val <- training(split2)
test <- testing(split2)

# Linear and log-linear models
M1 <- lm(Stress.Level ~ Quality.of.Sleep, train)
M2 <- lm(Stress.Level ~ log(Quality.of.Sleep), train)
M3 <- lm(Stress.Level ~ Quality.of.Sleep, val)
M4 <- lm(Stress.Level ~ log(Quality.of.Sleep), val)

# Quasipoisson models
M1_qpois <- glm(Stress.Level ~ Quality.of.Sleep, data = train, family = quasipoisson())
M2_qpois <- glm(Stress.Level ~ log(Quality.of.Sleep), data = train, family = quasipoisson())
M3_qpois <- glm(Stress.Level ~ Quality.of.Sleep, data = val, family = quasipoisson())
M4_qpois <- glm(Stress.Level ~ log(Quality.of.Sleep), data = val, family = quasipoisson())

# Overlay fits
ggplot(train, aes(x = Quality.of.Sleep, y = Stress.Level)) +
  geom_point(color = "black") +
  stat_smooth(method = "lm", se = FALSE, aes(color = "Linear")) +
  stat_smooth(method = "lm", formula = y ~ log(x), se = FALSE, aes(color = "Log")) +
  geom_line(aes(y = predict(M1_qpois, type = "response"), color = "Poisson Linear")) +
  geom_line(aes(y = predict(M2_qpois, type = "response"), color = "Poisson Log")) +
  labs(title = "Model Fits on Training Set", x = "Quality of Sleep", y = "Stress Level") +
  scale_color_manual(values = c("Linear" = "blue", "Log" = "green", "Poisson Linear" = "purple", "Poisson Log" = "orange")) +
  theme_minimal()

# RMSEs
rmse_M1 <- sqrt(mean(residuals(M1)^2))
rmse_M2 <- sqrt(mean(residuals(M2)^2))
rmse_M1_qpois <- sqrt(mean(residuals(M1_qpois)^2))
rmse_M2_qpois <- sqrt(mean(residuals(M2_qpois)^2))
rmse_M3_qpois <- sqrt(mean(residuals(M3_qpois)^2))
rmse_M4_qpois <- sqrt(mean(residuals(M4_qpois)^2))
rmse_M3 <- sqrt(mean((val$Stress.Level - predict(M3))^2))
rmse_M4 <- sqrt(mean((val$Stress.Level - predict(M4))^2))

model_names <- c("Linear", "Log-Linear", "Poisson (Linear)", "Poisson (Log)")
in_sample_rmse <- c(rmse_M1, rmse_M2, rmse_M1_qpois, rmse_M2_qpois)
out_sample_rmse <- c(rmse_M3, rmse_M4, rmse_M3_qpois, rmse_M4_qpois)
results_table <- data.frame(Model = model_names,
                            In_Sample_RMSE = round(in_sample_rmse, 4),
                            Out_of_Sample_RMSE = round(out_sample_rmse, 4))
print(results_table)

cat("Best model based on out-of-sample RMSE is:", 
    results_table$Model[which.min(results_table$Out_of_Sample_RMSE)], "\n")

# Final model test RMSE on test set
final_model <- glm(Stress.Level ~ Quality.of.Sleep, data = train, family = quasipoisson())
test_rmse <- sqrt(mean((test$Stress.Level - predict(final_model, newdata = test))^2))
cat("Test RMSE for final model (Poisson Linear):", round(test_rmse, 4), "\n")

# Multivariate Linear Model
model_lm <- lm(Stress.Level ~ Sleep.Duration + Age + Gender + Occupation + BMI.Category, data = train)
pred_train <- predict(model_lm, newdata = train)
pred_val <- predict(model_lm, newdata = val)
rmse_train <- sqrt(mean((train$Stress.Level - pred_train)^2))
rmse_val <- sqrt(mean((val$Stress.Level - pred_val)^2))
cat("Multivariate LM - In-sample RMSE:", round(rmse_train, 4), "\n")
cat("Multivariate LM - Out-of-sample RMSE:", round(rmse_val, 4), "\n")

# Multivariate Nonlinear Model (elastic net)
model_nl <- lm(Stress.Level ~ poly(Sleep.Duration, 2) + log(Age + 1) + Gender + Occupation + BMI.Category, data = train)
pred_nl_train <- predict(model_nl, newdata = train)
pred_nl_val <- predict(model_nl, newdata = val)
rmse_nl_train <- sqrt(mean((train$Stress.Level - pred_nl_train)^2))
rmse_nl_val <- sqrt(mean((val$Stress.Level - pred_nl_val)^2))
cat("Multivariate Nonlinear - In-sample RMSE:", round(rmse_nl_train, 4), "\n")
cat("Multivariate Nonlinear - Out-of-sample RMSE:", round(rmse_nl_val, 4), "\n")

# Elastic Net Regularization
combined <- rbind(train, val, test)
X_all <- model.matrix(Stress.Level ~ poly(Sleep.Duration, 2) + log(Age + 1) + Gender + Occupation + BMI.Category, data = combined)[, -1]

X_trainval <- X_all[1:(nrow(train) + nrow(val)), ]
y_trainval <- combined$Stress.Level[1:(nrow(train) + nrow(val))]
X_test <- X_all[(nrow(train) + nrow(val) + 1):nrow(combined), ]
y_test <- test$Stress.Level

cv_model <- cv.glmnet(X_trainval, y_trainval, alpha = 0.5)
best_lambda_enet <- cv_model$lambda.min
cat("Best lambda (Elastic Net):", best_lambda_enet, "\n")

plot(cv_model)

pred_enet_train <- predict(cv_model, s = best_lambda_enet, newx = X_trainval)
in_rmse_enet <- sqrt(mean((y_trainval - pred_enet_train)^2))

pred_test_enet <- predict(cv_model, s = best_lambda_enet, newx = X_test)
test_rmse_enet <- sqrt(mean((y_test - pred_test_enet)^2))
cat("Elastic Net - Test RMSE:", round(test_rmse_enet, 4), "\n")

# 5c Model RMSE Summary Table
model_5c_names <- c("Multivariate Linear", "Multivariate Nonlinear", "Elastic Net")
in_rmse_5c <- c(rmse_train, rmse_nl_train, in_rmse_enet)
out_rmse_5c <- c(rmse_val, rmse_nl_val, test_rmse_enet)
summary_5c <- data.frame(Model = model_5c_names,
                         In_Sample_RMSE = round(in_rmse_5c, 4),
                         Out_of_Sample_RMSE = round(out_rmse_5c, 4))
print(summary_5c)

# Load SVM library
library(e1071)

# Fit SVM model (using radial kernel by default)
svm_model <- svm(Stress.Level ~ Sleep.Duration + Age + Gender + Occupation + BMI.Category,
                 data = train,
                 kernel = "radial",
                 cost = 1,   # C parameter
                 epsilon = 0.1)

# Predict on training and validation sets
pred_svm_train <- predict(svm_model, newdata = train)
pred_svm_val <- predict(svm_model, newdata = val)

# Compute RMSE
rmse_svm_train <- sqrt(mean((train$Stress.Level - pred_svm_train)^2))
rmse_svm_val <- sqrt(mean((val$Stress.Level - pred_svm_val)^2))

# Output
cat("SVM - In-sample RMSE:", round(rmse_svm_train, 4), "\n")
cat("SVM - Out-of-sample RMSE:", round(rmse_svm_val, 4), "\n")

summary_5c <- data.frame(
  Model = c("Multivariate Linear", "Multivariate Nonlinear", "Elastic Net", "SVM"),
  In_Sample_RMSE = round(c(rmse_train, rmse_nl_train, in_rmse_enet, rmse_svm_train), 4),
  Out_of_Sample_RMSE = round(c(rmse_val, rmse_nl_val, test_rmse_enet, rmse_svm_val), 4)
)

print(summary_5c)

cat("Best model based on out-of-sample RMSE:", 
    summary_5c$Model[which.min(summary_5c$Out_of_Sample_RMSE)], "\n")

# Define parameter grid to TUNE SVM model
tune_result <- tune(
  svm,
  Stress.Level ~ Sleep.Duration + Age + Gender + Occupation + BMI.Category,
  data = train,
  ranges = list(
    cost = c(0.1, 1, 10, 100),
    gamma = c(0.01, 0.1, 1),
    epsilon = c(0.01, 0.1, 0.5)
  )
)

# Best model
best_svm <- tune_result$best.model

# Predict on validation set
svm_val_preds_tuned <- predict(best_svm, newdata = val)
rmse_svm_val_tuned <- sqrt(mean((val$Stress.Level - svm_val_preds_tuned)^2))

cat("Tuned SVM - Out-of-sample RMSE:", round(rmse_svm_val_tuned, 4), "\n")

# Assign distinct names to tuned SVM RMSEs (if not done yet)
rmse_svm_train_tuned <- sqrt(mean((train$Stress.Level - predict(best_svm, newdata = train))^2))
rmse_svm_val_tuned <- sqrt(mean((val$Stress.Level - predict(best_svm, newdata = val))^2))

# Update table with correct values
model_5c_names <- c("Multivariate Linear", 
                    "Multivariate Nonlinear", 
                    "Elastic Net", 
                    "SVM", 
                    "Tuned SVM")

in_rmse_5c <- c(rmse_train, 
                rmse_nl_train, 
                in_rmse_enet, 
                rmse_svm_train,        # original SVM
                rmse_svm_train_tuned)  # tuned SVM

out_rmse_5c <- c(rmse_val, 
                 rmse_nl_val, 
                 test_rmse_enet, 
                 rmse_svm_val,         # original SVM
                 rmse_svm_val_tuned)   # tuned SVM

# Rebuild and print table
summary_5c <- data.frame(
  Model = model_5c_names,
  In_Sample_RMSE = round(in_rmse_5c, 4),
  Out_of_Sample_RMSE = round(out_rmse_5c, 4)
)

print(summary_5c)

cat("Best model based on out-of-sample RMSE:", 
    summary_5c$Model[which.min(summary_5c$Out_of_Sample_RMSE)], "\n")

library(tibble)  # ensure it's loaded

#SPECIFYING THE REGRESSION TREE MODEL
reg_spec <- decision_tree(min_n = 20 , #minimum number of observations for split
                          tree_depth = 30, #max tree depth
                          cost_complexity = 0.01)  %>% #regularization parameter
  set_engine("rpart") %>%
  set_mode("regression")
print(reg_spec)

#ESTIMATING THE MODEL (CAN BE DONE IN ONE STEP ABOVE WITH EXTRA %>%)
reg_fmla <- Stress.Level ~ .
reg_tree <- reg_spec %>%
  fit(formula = reg_fmla, data = train)
print(reg_tree)

# Predict on training data
train_preds <- predict(reg_tree, new_data = train) %>%
  bind_cols(as_tibble(train) %>% dplyr::select(Stress.Level))

# Calculate in-sample metrics
train_metrics <- train_preds %>%
  metrics(truth = Stress.Level, estimate = .pred)

print("In-Sample Performance:")
print(train_metrics)


# Loop through all columns in test that are also in train
for (col in names(test)) {
  if (is.factor(train[[col]])) {
    test[[col]] <- factor(test[[col]], levels = levels(train[[col]]))
  }
}

test$Occupation <- factor(test$Occupation, levels = levels(train$Occupation))

levels(train$Occupation)
levels(test$Occupation)

for (col in names(train)) {
  if (is.factor(train[[col]])) {
    test[[col]] <- factor(test[[col]], levels = levels(train[[col]]))
  }
}

train$Occupation <- as.factor(train$Occupation)
test$Occupation <- as.factor(test$Occupation)

# Predict on test data
test_preds <- predict(reg_tree, new_data = test) %>%
  bind_cols(data.frame(Stress.Level = test$Stress.Level))

# Calculate out-of-sample metrics
test_metrics <- test_preds %>%
  metrics(truth = Stress.Level, estimate = .pred)

print("Out-of-Sample Performance:")
print(test_metrics)

install.packages("rpart.plot")  # Run only once if not already installed
library(rpart.plot)

# Plot the tree
rpart.plot(reg_tree$fit,
           type = 4,        # Fancy split labels
           extra = 101,     # Show fitted values and % of observations
           fallen.leaves = TRUE,
           roundint = FALSE,
           main = "Regression Tree for Stress Level",
           cex = .7)  # Bigger text

rf_spec <- rand_forest(
  mode = "regression",
  mtry = 3,              # Number of predictors to try at each split
  trees = 500,           # Number of trees
  min_n = 5              # Minimum samples per node
) %>%
  set_engine("ranger")   # Use the fast "ranger" engine
rf_model <- rf_spec %>%
  fit(Stress.Level ~ ., data = train)

# Make predictions
train_preds <- predict(rf_model, new_data = train) %>%
  bind_cols(data.frame(Stress.Level = train$Stress.Level))

test_preds <- predict(rf_model, new_data = test) %>%
  bind_cols(data.frame(Stress.Level = test$Stress.Level))

# In-sample performance
train_metrics <- train_preds %>%
  metrics(truth = Stress.Level, estimate = .pred)

# Out-of-sample performance
test_metrics <- test_preds %>%
  metrics(truth = Stress.Level, estimate = .pred)

print("In-Sample Performance:")
print(train_metrics)

print("Out-of-Sample Performance:")
print(test_metrics)
#TUNING ENSEMBLE MODEl
rf_spec <- rand_forest(
  mode = "regression",
  mtry = tune(),      # to be tuned
  min_n = tune(),
  trees = 500
) %>%
  set_engine("ranger")

set.seed(123)
cv_folds <- vfold_cv(train, v = 5)

rf_workflow <- workflow() %>%
  add_model(rf_spec) %>%
  add_formula(Stress.Level ~ .)

detach("package:Metrics", unload = TRUE)
library(yardstick)  # Make sure it's loaded after detaching Metrics
metric_set(rmse)

rf_grid <- grid_random(
  mtry(range = c(1, ncol(train) - 1)),
  min_n(range = c(2, 20)),
  size = 20  # number of combinations to try
)

tuned_results <- tune_grid(
  rf_workflow,
  resamples = cv_folds,
  grid = rf_grid,
  metrics = metric_set(rmse)
)

best_params <- select_best(tuned_results, metric = "rmse")

final_rf <- finalize_model(rf_spec, best_params)

final_workflow <- rf_workflow %>%
  finalize_workflow(best_params)

# Fit on full training data
final_model <- fit(final_workflow, data = train)

test_preds <- predict(final_model, new_data = test) %>%
  bind_cols(data.frame(Stress.Level = test$Stress.Level))

test_metrics <- test_preds %>%
  metrics(truth = Stress.Level, estimate = .pred)

print(test_metrics)

# Extract RMSE values from yardstick metrics tibbles
tree_test_rmse <- test_metrics %>% filter(.metric == "rmse") %>% pull(.estimate)
tree_train_rmse <- train_metrics %>% filter(.metric == "rmse") %>% pull(.estimate)

# You’ll also want to extract RMSE for final tuned random forest
final_test_preds <- predict(final_model, new_data = test) %>%
  bind_cols(data.frame(Stress.Level = test$Stress.Level))

final_rf_rmse <- final_test_preds %>%
  metrics(truth = Stress.Level, estimate = .pred) %>%
  filter(.metric == "rmse") %>%
  pull(.estimate)

# Predict on training data (in-sample)
train_preds_rf <- predict(final_model, new_data = train) %>%
  bind_cols(data.frame(Stress.Level = train$Stress.Level))
# Calculate in-sample RMSE for the final RF model
train_rmse_rf <- train_preds_rf %>%
  metrics(truth = Stress.Level, estimate = .pred) %>%
  filter(.metric == "rmse") %>%
  pull(.estimate)
# Predict on validation data (validation RMSE)
val_preds_rf <- predict(final_model, new_data = val) %>%
  bind_cols(data.frame(Stress.Level = val$Stress.Level))
# Calculate validation RMSE for the final RF model
val_rmse_rf <- val_preds_rf %>%
  metrics(truth = Stress.Level, estimate = .pred) %>%
  filter(.metric == "rmse") %>%
  pull(.estimate)
model_performance <- tibble(
  model_5c_names <- c("Multivariate Linear", 
                      "Multivariate Nonlinear", 
                      "Elastic Net", 
                      "SVM", 
                      "Tuned SVM",
                      "Regression Tree", 
                      "Tuned Random Forest"),
  
  in_rmse_5c <- c(rmse_train, 
                  rmse_nl_train, 
                  in_rmse_enet, 
                  rmse_svm_train,        # original SVM
                  rmse_svm_train_tuned,
                  tree_train_rmse,         
                  train_rmse_rf),  # tuned SVM
  
  out_rmse_5c <- c(rmse_val, 
                   rmse_nl_val, 
                   test_rmse_enet, 
                   rmse_svm_val,         # original SVM
                   rmse_svm_val_tuned,
                   tree_test_rmse,  
                   val_rmse_rf),  
  Test_RMSE = round(c(NA, NA, NA, NA, NA, tree_test_rmse, final_rf_rmse), 4))   # tuned SVM

print(model_performance)

# Report the selected final model and its test performance
cat("\n✅ Final model selected based on validation RMSE: Tuned Random Forest\n")
cat("📉 Test RMSE (uncontaminated out-of-sample error):", round(final_rf_rmse, 4), "\n")

# Task #6, Classification 
# Changing stess level to catagorical variable (Stress.Category)
library(dplyr)

df <- df %>%
  mutate(Stress.Category = case_when(
    Stress.Level < 6  ~ "Low",
    Stress.Level == 6 ~ "Average",
    Stress.Level > 6  ~ "High"
  ))

df <- df %>%
  mutate(Stress.Category = factor(Stress.Category, levels = c("Low", "Average", "High")))

library(tidymodels)

# Partition data using Stress.Category for stratified sampling
set.seed(123)
split1 <- initial_split(df, prop = 0.7, strata = Stress.Category)
train <- training(split1)
holdout <- testing(split1)

split2 <- initial_split(holdout, prop = 0.5, strata = Stress.Category)
val <- training(split2)
test <- testing(split2)

# Load MASS package for ordinal logistic regression
library(MASS)

# 1. Find the most common (majority) class in the training set
majority_class <- names(sort(table(train$Stress.Category), decreasing = TRUE))[1]
cat("Majority class is:", majority_class, "\n")

# 2. Predict the majority class for every observation in the validation set
baseline_preds <- factor(rep(majority_class, nrow(val)),
                         levels = levels(val$Stress.Category))

# 3. Calculate accuracy of baseline model
baseline_accuracy <- mean(baseline_preds == val$Stress.Category)
cat("Baseline model accuracy (always predicts", majority_class, "):", baseline_accuracy, "\n")

# Probit model
probit_model <- glm(Stress.Category ~ Quality.of.Sleep + Age + Physical.Activity.Level, 
                    data = train, 
                    family = binomial(link = "probit"))

# Fit ordinal logistic regression model with multiple numeric variables
ord_logit_model_activity <- polr(
  Stress.Category ~ Quality.of.Sleep + Age + Physical.Activity.Level,
  data = train,
  Hess = TRUE
)

# View model summary
summary(ord_logit_model_activity)
summary(probit_model)

# Predict on validation set
val_preds_activity <- predict(ord_logit_model_activity, newdata = val)
val_preds_activity_probit <- predict(probit_model, newdata = val)

# Compute AUC
roc_logit <- roc(Stress.Category, ord_logit_model_activity)
roc_probit <- roc(Stress.Category, probit_model)

auc(roc_logit)
auc(roc_probit)

# Optional: Plot ROC curves
plot(roc_logit, col = "blue", main = "ROC Curves: Logit vs Probit")
lines(roc_probit, col = "red")
legend("bottomright", legend = c("Logit", "Probit"), col = c("blue", "red"), lwd = 2)

# Accuracy
mean(val_preds_activity == val$Stress.Category)
#confusion Matrix
table(Predicted = val_preds_activity, Actual = val$Stress.Category)

# Try with fewer predictors to test convergence
ord_logit_model_simple <- polr(Stress.Category ~ Quality.of.Sleep + Age,
                               data = train,
                               Hess = TRUE)

summary(ord_logit_model_simple)

# Predict on validation set
val_preds <- predict(ord_logit_model_simple, newdata = val)

# Define SVM model
svm_spec <- svm_rbf(mode = "classification") %>%
  set_engine("kernlab")

# Fit the model
svm_fit <- workflow() %>%
  add_model(svm_spec) %>%
  add_formula(Stress.Category ~ Quality.of.Sleep + Age + Physical.Activity.Level) %>%
  fit(data = train)

# In-sample prediction for SVM
train_preds <- predict(svm_fit, new_data = train) %>%
  bind_cols(train %>% select(Stress.Category))

# Out-of-sample (validation set) prediction for SVM
val_preds <- predict(svm_fit, new_data = val) %>%
  bind_cols(val %>% select(Stress.Category))

# Accuracy for SVM
train_acc <- train_preds %>%
  metrics(truth = Stress.Category, estimate = .pred_class) %>%
  filter(.metric == "accuracy")

val_acc <- val_preds %>%
  metrics(truth = Stress.Category, estimate = .pred_class) %>%
  filter(.metric == "accuracy")

# Define SVM model with tuning
svm_tune_spec <- svm_rbf(
  mode = "classification",
  cost = tune(),
  rbf_sigma = tune()
) %>%
  set_engine("kernlab")

# Create SVM workflow
svm_workflow <- workflow() %>%
  add_model(svm_tune_spec) %>%
  add_formula(Stress.Category ~ Quality.of.Sleep + Age + Physical.Activity.Level)

# Define tuning grid
svm_grid <- grid_regular(cost(), rbf_sigma(), levels = 5)

# Perform tuning
set.seed(123)
svm_res <- tune_grid(
  svm_workflow,
  resamples = vfold_cv(train, v = 5, strata = Stress.Category),
  grid = svm_grid,
  metrics = metric_set(accuracy)
)

# Select best parameters
best_svm <- select_best(svm_res, "accuracy")

# Finalize model with best parameters
final_svm <- finalize_workflow(svm_workflow, best_svm) %>%
  fit(data = train)

# Predict on validation set with tuned model
val_preds_tuned <- predict(final_svm, new_data = val) %>%
  bind_cols(val %>% select(Stress.Category))

val_acc_tuned <- val_preds_tuned %>%
  metrics(truth = Stress.Category, estimate = .pred_class) %>%
  filter(.metric == "accuracy")

# Accuracy
mean(val_preds == val$Stress.Category)
#confusion Matrix
table(Predicted = val_preds, Actual = val$Stress.Category)
####*****Classification Tree#########
# Define recipe
tree_rec <- recipe(Stress.Category ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%  # if needed
  step_zv(all_predictors())                 # remove zero variance

# Classification tree model
tree_mod <- decision_tree(mode = "classification") %>%
  set_engine("rpart")

# Workflow
tree_wf <- workflow() %>%
  add_model(tree_mod) %>%
  add_recipe(tree_rec)

# Fit to training data
tree_fit <- tree_wf %>% fit(data = train_data)

# Predict on training
train_preds <- predict(tree_fit, train_data, type = "class") %>%
  bind_cols(train_data)

# Predict on test
test_preds <- predict(tree_fit, test_data, type = "class") %>%
  bind_cols(test_data)

# Evaluate in-sample and out-of-sample
train_metrics <- train_preds %>%
  metrics(truth = Stress.Category, estimate = .pred_class)

test_metrics <- test_preds %>%
  metrics(truth = Stress.Category, estimate = .pred_class)

print(train_metrics)
print(test_metrics)

# Extract tree for plotting
tree_model <- extract_fit_parsnip(tree_fit)$fit
rpart.plot(tree_model)

# Model with tuning
tree_mod_tuned <- decision_tree(
  mode = "classification",
  cost_complexity = tune(),
  tree_depth = tune()
) %>% set_engine("rpart")

# Workflow
tree_wf_tuned <- workflow() %>%
  add_model(tree_mod_tuned) %>%
  add_recipe(tree_rec)

# 5-fold CV
tree_folds <- vfold_cv(train_data, v = 5, strata = Stress.Category)

# Grid search
tree_grid <- grid_regular(
  cost_complexity(),
  tree_depth(),
  levels = 5
)

# Tuning
tree_tune_res <- tune_grid(
  tree_wf_tuned,
  resamples = tree_folds,
  grid = tree_grid,
  metrics = metric_set(accuracy, roc_auc)
)

# Best model
best_tree <- select_best(tree_tune_res, "accuracy")
final_tree <- finalize_workflow(tree_wf_tuned, best_tree)
final_fit <- final_tree %>% fit(train_data)

# Final test set predictions
final_preds <- predict(final_fit, test_data, type = "class") %>%
  bind_cols(test_data)

final_metrics <- final_preds %>%
  metrics(truth = Stress.Category, estimate = .pred_class)

print(final_metrics)


####Tree Model####
# Recipe
rf_rec <- recipe(Stress.Category ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

# Model (initial, untuned)
rf_mod <- rand_forest(mode = "classification") %>%
  set_engine("ranger")

# Workflow
rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_mod)

# Fit model to training data
rf_fit <- rf_wf %>% fit(data = train_data)

# Predictions
train_preds_rf <- predict(rf_fit, train_data, type = "class") %>%
  bind_cols(train_data)

test_preds_rf <- predict(rf_fit, test_data, type = "class") %>%
  bind_cols(test_data)

# Accuracy and ROC AUC
train_metrics_rf <- train_preds_rf %>%
  metrics(truth = Stress.Category, estimate = .pred_class)

test_metrics_rf <- test_preds_rf %>%
  metrics(truth = Stress.Category, estimate = .pred_class)

print(train_metrics_rf)
print(test_metrics_rf)

# Model with tuning parameters
rf_mod_tuned <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = 500,
  min_n = tune()
) %>% set_engine("ranger")

# Workflow
rf_wf_tuned <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_mod_tuned)

# Cross-validation folds
rf_folds <- vfold_cv(train_data, v = 5, strata = Stress.Category)

# Grid search
rf_grid <- grid_regular(
  mtry(range = c(2, 10)),
  min_n(range = c(2, 10)),
  levels = 5
)

# Tune the model
rf_tune_res <- tune_grid(
  rf_wf_tuned,
  resamples = rf_folds,
  grid = rf_grid,
  metrics = metric_set(accuracy, roc_auc)
)

# Get best model by accuracy
best_rf <- select_best(rf_tune_res, "accuracy")

# Final model
final_rf_wf <- finalize_workflow(rf_wf_tuned, best_rf)
final_rf_fit <- final_rf_wf %>% fit(train_data)

# Predict on test set
final_rf_preds <- predict(final_rf_fit, test_data, type = "class") %>%
  bind_cols(test_data)

final_rf_metrics <- final_rf_preds %>%
  metrics(truth = Stress.Category, estimate = .pred_class)

print(final_rf_metrics)

model_names <- c(
  "Logistic Regression",
  "Probit Model",
  "SVM (Untuned)",
  "SVM (Tuned)",
  "Classification Tree",
  "Tree (Tuned)",
  "Random Forest (Untuned)",
  "Random Forest (Tuned)"
)

in_sample <- c(
  acc_logit_train,
  acc_probit_train,
  acc_svm_train,
  acc_svm_tuned_train,
  acc_tree_train,
  acc_tree_tuned_train,
  acc_rf_train,
  acc_rf_tuned_train
)

out_of_sample <- c(
  acc_logit_val,
  acc_probit_val,
  acc_svm_val,
  acc_svm_tuned_val,
  acc_tree_val,
  acc_tree_tuned_val,
  acc_rf_val,
  acc_rf_tuned_val
)

test_performance <- c(
  acc_logit_test,
  acc_probit_test,
  acc_svm_test,
  acc_svm_tuned_test,
  acc_tree_test,
  acc_tree_tuned_test,
  acc_rf_test,
  acc_rf_tuned_test
)

# Create the summary table
performance_table <- tibble(
  Model = model_names,
  In_Sample_Accuracy = round(in_sample, 4),
  Validation_Accuracy = round(out_of_sample, 4),
  Test_Accuracy = round(test_performance, 4)
)

print(performance_table)






